{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train MNIST images :  55000\n",
      "Total test MNIST images:  10000\n"
     ]
    }
   ],
   "source": [
    "# total amount of train and test sets\n",
    "print 'Total train MNIST images : ', len(mnist_data.train.images)\n",
    "print 'Total test MNIST images: ', len(mnist_data.test.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When it comes to classifying images via RNN, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 steps for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "total_epoch = 200000\n",
    "\n",
    "# Generally, the smaller the batch size the noisier the updates, \n",
    "# so if you decrease the batch size you should probably decrease the learning rate, \n",
    "# and train for more iterations.\n",
    "batch_size = 256\n",
    "\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "# MNIST data input (img shape: 28*28)\n",
    "total_input = 28 \n",
    "\n",
    "# total timestep\n",
    "total_timestep = 28\n",
    "\n",
    "# hidden layer num of features\n",
    "total_hidden = 256\n",
    "\n",
    "# MNIST total classes (0-9 digits)\n",
    "total_classes = 10 \n",
    "\n",
    "# tensorflow graph input\n",
    "x = tf.placeholder(\"float\", [None, total_timestep, total_input])\n",
    "y = tf.placeholder(\"float\", [None, total_classes])\n",
    "\n",
    "# define model weight and bias\n",
    "W = {'weight': tf.Variable(tf.random_normal([total_hidden, total_classes]))}\n",
    "\n",
    "b = {'bias': tf.Variable(tf.random_normal([total_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, W, b):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, total_timestep, total_input)\n",
    "    # Required shape: 'total_timestep' tensors list of shape (batch_size, total_input)\n",
    "    \n",
    "    # Permuting batch_size and total timestep\n",
    "    x = tf.transpose(x, perm=[1, 0, 2])\n",
    "    \n",
    "    # Reshaping to (total_timestep*batch_size, total_input)\n",
    "    x = tf.reshape(x, [-1, total_input])\n",
    "    \n",
    "    # Split to get a list of 'total_timestep' tensors of shape (batch_size, total_input)\n",
    "    x = tf.split(0, total_timestep, x)\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(total_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], W['weight']) + b['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_model = RNN(x, W, b)\n",
    "\n",
    "# define loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(rnn_model, y))\n",
    "\n",
    "# adam optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# model accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(rnn_model,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the RNN model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 2560, Minibatch Loss= 1.709867, Training Accuracy= 0.39844\n",
      "Iterations: 5120, Minibatch Loss= 1.086316, Training Accuracy= 0.69141\n",
      "Iterations: 7680, Minibatch Loss= 1.091629, Training Accuracy= 0.62109\n",
      "Iterations: 10240, Minibatch Loss= 0.749779, Training Accuracy= 0.76953\n",
      "Iterations: 12800, Minibatch Loss= 0.875684, Training Accuracy= 0.71484\n",
      "Iterations: 15360, Minibatch Loss= 0.555176, Training Accuracy= 0.82031\n",
      "Iterations: 17920, Minibatch Loss= 0.425183, Training Accuracy= 0.86328\n",
      "Iterations: 20480, Minibatch Loss= 0.287345, Training Accuracy= 0.89844\n",
      "Iterations: 23040, Minibatch Loss= 0.240682, Training Accuracy= 0.90234\n",
      "Iterations: 25600, Minibatch Loss= 0.317411, Training Accuracy= 0.87500\n",
      "Iterations: 28160, Minibatch Loss= 0.293748, Training Accuracy= 0.90234\n",
      "Iterations: 30720, Minibatch Loss= 0.324046, Training Accuracy= 0.87500\n",
      "Iterations: 33280, Minibatch Loss= 0.200775, Training Accuracy= 0.94922\n",
      "Iterations: 35840, Minibatch Loss= 0.222271, Training Accuracy= 0.92578\n",
      "Iterations: 38400, Minibatch Loss= 0.191186, Training Accuracy= 0.94141\n",
      "Iterations: 40960, Minibatch Loss= 0.326234, Training Accuracy= 0.90625\n",
      "Iterations: 43520, Minibatch Loss= 0.177295, Training Accuracy= 0.94922\n",
      "Iterations: 46080, Minibatch Loss= 0.130172, Training Accuracy= 0.94922\n",
      "Iterations: 48640, Minibatch Loss= 0.234079, Training Accuracy= 0.92578\n",
      "Iterations: 51200, Minibatch Loss= 0.128238, Training Accuracy= 0.95312\n",
      "Iterations: 53760, Minibatch Loss= 0.082963, Training Accuracy= 0.98438\n",
      "Iterations: 56320, Minibatch Loss= 0.191977, Training Accuracy= 0.95312\n",
      "Iterations: 58880, Minibatch Loss= 0.111276, Training Accuracy= 0.96484\n",
      "Iterations: 61440, Minibatch Loss= 0.227304, Training Accuracy= 0.92578\n",
      "Iterations: 64000, Minibatch Loss= 0.114711, Training Accuracy= 0.97266\n",
      "Iterations: 66560, Minibatch Loss= 0.216714, Training Accuracy= 0.92578\n",
      "Iterations: 69120, Minibatch Loss= 0.088619, Training Accuracy= 0.96875\n",
      "Iterations: 71680, Minibatch Loss= 0.120663, Training Accuracy= 0.96094\n",
      "Iterations: 74240, Minibatch Loss= 0.060757, Training Accuracy= 0.98438\n",
      "Iterations: 76800, Minibatch Loss= 0.123950, Training Accuracy= 0.96094\n",
      "Iterations: 79360, Minibatch Loss= 0.126809, Training Accuracy= 0.96094\n",
      "Iterations: 81920, Minibatch Loss= 0.074324, Training Accuracy= 0.97266\n",
      "Iterations: 84480, Minibatch Loss= 0.090107, Training Accuracy= 0.96094\n",
      "Iterations: 87040, Minibatch Loss= 0.070217, Training Accuracy= 0.98047\n",
      "Iterations: 89600, Minibatch Loss= 0.083988, Training Accuracy= 0.97656\n",
      "Iterations: 92160, Minibatch Loss= 0.111029, Training Accuracy= 0.94531\n",
      "Iterations: 94720, Minibatch Loss= 0.064645, Training Accuracy= 0.97656\n",
      "Iterations: 97280, Minibatch Loss= 0.094577, Training Accuracy= 0.96875\n",
      "Iterations: 99840, Minibatch Loss= 0.119970, Training Accuracy= 0.96484\n",
      "Iterations: 102400, Minibatch Loss= 0.119398, Training Accuracy= 0.97266\n",
      "Iterations: 104960, Minibatch Loss= 0.141797, Training Accuracy= 0.95312\n",
      "Iterations: 107520, Minibatch Loss= 0.070706, Training Accuracy= 0.96875\n",
      "Iterations: 110080, Minibatch Loss= 0.102655, Training Accuracy= 0.97656\n",
      "Iterations: 112640, Minibatch Loss= 0.099945, Training Accuracy= 0.96875\n",
      "Iterations: 115200, Minibatch Loss= 0.083363, Training Accuracy= 0.96484\n",
      "Iterations: 117760, Minibatch Loss= 0.073520, Training Accuracy= 0.97656\n",
      "Iterations: 120320, Minibatch Loss= 0.108903, Training Accuracy= 0.96484\n",
      "Iterations: 122880, Minibatch Loss= 0.100830, Training Accuracy= 0.95703\n",
      "Iterations: 125440, Minibatch Loss= 0.123200, Training Accuracy= 0.96875\n",
      "Iterations: 128000, Minibatch Loss= 0.077466, Training Accuracy= 0.97656\n",
      "Iterations: 130560, Minibatch Loss= 0.129302, Training Accuracy= 0.95312\n",
      "Iterations: 133120, Minibatch Loss= 0.062360, Training Accuracy= 0.98047\n",
      "Iterations: 135680, Minibatch Loss= 0.049220, Training Accuracy= 0.98047\n",
      "Iterations: 138240, Minibatch Loss= 0.060445, Training Accuracy= 0.98828\n",
      "Iterations: 140800, Minibatch Loss= 0.089746, Training Accuracy= 0.96484\n",
      "Iterations: 143360, Minibatch Loss= 0.045012, Training Accuracy= 0.99219\n",
      "Iterations: 145920, Minibatch Loss= 0.064684, Training Accuracy= 0.96875\n",
      "Iterations: 148480, Minibatch Loss= 0.086286, Training Accuracy= 0.98047\n",
      "Iterations: 151040, Minibatch Loss= 0.063912, Training Accuracy= 0.97266\n",
      "Iterations: 153600, Minibatch Loss= 0.052770, Training Accuracy= 0.98047\n",
      "Iterations: 156160, Minibatch Loss= 0.130058, Training Accuracy= 0.96875\n",
      "Iterations: 158720, Minibatch Loss= 0.127206, Training Accuracy= 0.96094\n",
      "Iterations: 161280, Minibatch Loss= 0.070770, Training Accuracy= 0.97656\n",
      "Iterations: 163840, Minibatch Loss= 0.062481, Training Accuracy= 0.98047\n",
      "Iterations: 166400, Minibatch Loss= 0.062740, Training Accuracy= 0.98047\n",
      "Iterations: 168960, Minibatch Loss= 0.022949, Training Accuracy= 0.99609\n",
      "Iterations: 171520, Minibatch Loss= 0.044113, Training Accuracy= 0.98047\n",
      "Iterations: 174080, Minibatch Loss= 0.053516, Training Accuracy= 0.98828\n",
      "Iterations: 176640, Minibatch Loss= 0.027809, Training Accuracy= 0.99219\n",
      "Iterations: 179200, Minibatch Loss= 0.048610, Training Accuracy= 0.98438\n",
      "Iterations: 181760, Minibatch Loss= 0.030151, Training Accuracy= 0.98828\n",
      "Iterations: 184320, Minibatch Loss= 0.099230, Training Accuracy= 0.98438\n",
      "Iterations: 186880, Minibatch Loss= 0.069554, Training Accuracy= 0.97656\n",
      "Iterations: 189440, Minibatch Loss= 0.069440, Training Accuracy= 0.97656\n",
      "Iterations: 192000, Minibatch Loss= 0.096559, Training Accuracy= 0.97656\n",
      "Iterations: 194560, Minibatch Loss= 0.053175, Training Accuracy= 0.98438\n",
      "Iterations: 197120, Minibatch Loss= 0.051675, Training Accuracy= 0.98438\n",
      "Iterations: 199680, Minibatch Loss= 0.038779, Training Accuracy= 0.98828\n",
      "Training is complete!\n",
      "\n",
      "Test Accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    \n",
    "    # training starts here!\n",
    "    while step * batch_size < total_epoch:\n",
    "        batch_x, batch_y = mnist_data.train.next_batch(batch_size)\n",
    "        \n",
    "        # transform data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, total_timestep, total_input))\n",
    "        \n",
    "        # execute optimization (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print 'Iterations: ' + str(step*batch_size) + ', Minibatch Loss= ' + \\\n",
    "                  '{:.6f}'.format(loss) + ', Training Accuracy= ' + \\\n",
    "                  '{:.5f}'.format(acc)\n",
    "        step += 1  \n",
    "    print 'Training is complete!'\n",
    "    print ''\n",
    "    \n",
    "    # calculate accuracy for 10000 mnist test images\n",
    "    test_len = 10000\n",
    "    test_data = mnist_data.test.images[:test_len].reshape((-1, total_timestep, total_input))\n",
    "    test_label = mnist_data.test.labels[:test_len]\n",
    "    \n",
    "    print 'Test Accuracy:', sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
